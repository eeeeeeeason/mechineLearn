
  - 损失函数的最小值，即是简单线性回归中的最优函数(y^i - ax^i + b )^2 计算此最小值
  - 评测标准
    - mse_test = np.sum((y_predict- y_test)**2) / len(y_test)
    - rmse = sqrt(mse_test)
    - mae = np.sum(np.absolute(y_predict - y_test)) / len(y_test)
    - R squared = 1时为最优，  <=0时不适用 R^2 = 1 - MSE(yhat,y) / Var(y)   此法目前是最优的评测标准
    
  ### 多元线性回归
    - 核心在于从二维的一个ax+b 变成多维的 theat0 + theta1x1 + theta2x2 .... 需要将这两部分拆分为两个向量,行向量为X第一列补充1 列向量为theta0...thetan
    ```py
      """coef 对应一维的a, interception对应b , 多维时 coef代表向量[theta1,....thetan]除了theta0, interception代表theta0"""
      """生成一列为1的向量，长度与训练数据相同进行合并，目的是为了凑theta0的X，使每一项都存在X"""
      X_b = np.hstack([np.ones((len(X_train),1)), X_train]) 
      self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)
      self.interception_ = self._theta[0]
      self.coef_ = self._theta[1:]
    ```
    - 计算得出的theta这个模型得分r2方程组
    ```py
      def score(self, X_test, y_test):
        y_predict = self.predict(X_test)
        return 1 - mean_squared_error(y_test, y_predict) / np.var(y_test)
    ```
    
