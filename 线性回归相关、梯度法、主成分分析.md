
  - 损失函数的最小值，即是简单线性回归中的最优函数(y^i - ax^i + b )^2 计算此最小值
  - 评测标准
    - mse_test = np.sum((y_predict- y_test)**2) / len(y_test)
    - rmse = sqrt(mse_test)
    - mae = np.sum(np.absolute(y_predict - y_test)) / len(y_test)
    - R squared = 1时为最优，  <=0时不适用 R^2 = 1 - MSE(yhat,y) / Var(y)   此法目前是最优的评测标准
    
  ### 多元线性回归
    - 核心在于从二维的一个ax+b 变成多维的 theat0 + theta1x1 + theta2x2 .... 需要将这两部分拆分为两个向量,行向量为X第一列补充1 列向量为theta0...thetan
    ```py
      """coef 对应一维的a, interception对应b , 多维时 coef代表向量[theta1,....thetan]除了theta0, interception代表theta0"""
      """生成一列为1的向量，长度与训练数据相同进行合并，目的是为了凑theta0的X，使每一项都存在X"""
      X_b = np.hstack([np.ones((len(X_train),1)), X_train]) 
      self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)
      self.interception_ = self._theta[0]
      self.coef_ = self._theta[1:]
    ```
    - 计算得出的theta这个模型得分r2方程组
    ```py
      def score(self, X_test, y_test):
        y_predict = self.predict(X_test)
        return 1 - mean_squared_error(y_test, y_predict) / np.var(y_test)
    ```
    
  ### 梯度下降法
    - 作用，最小化损失函数的搜索方法
    - 对y=损失函数与x=theta模型的求导获取损失函数的极值点最小值时
    - 对y=损失函数与x=theta模型的求导，导数可代表方向，导数为负表示损失函数在X轴负方向上递增
    - eta表示步长 (学习率，梯度下降法的超参数)，-eta * dJ * dtheta 表示单位步长。eta太小影响计算效率，太大导致损失函数反而上升没有计算结果
    - 并不是所有y=损失函数与x=theta模型都有唯一的极小值，初始点也是梯度下降的超参数
      - 解决方法： 每次随机初始点，多次运行
    - 向量化和数据标准化：数据的单位要以归一法进行处理
    - 梯度下降法的优化。可用随机梯度下降法进行
     
  ### 主成分分析法
    - 主要作用，降维，去噪，可视化
    - 步骤1: 样本均值归0 demean
    - 步骤2: 方差最大值
  ### 梯度上升法
    - 
