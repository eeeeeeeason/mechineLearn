
  - 损失函数的最小值，即是简单线性回归中的最优函数(y^i - ax^i + b )^2 计算此最小值
  - 评测标准
    - mse_test = np.sum((y_predict- y_test)**2) / len(y_test)
    - rmse = sqrt(mse_test)
    - mae = np.sum(np.absolute(y_predict - y_test)) / len(y_test)
    - R squared = 1时为最优，  <=0时不适用 R^2 = 1 - MSE(yhat,y) / Var(y)   此法目前是最优的评测标准
    
  ### 多元线性回归
    - 核心在于从二维的一个ax+b 变成多维的 theat0 + theta1x1 + theta2x2 .... 需要将这两部分拆分为两个向量,行向量为X第一列补充1 列向量为theta0...thetan
    ```py
      """coef 对应一维的a, interception对应b , 多维时 coef代表向量[theta1,....thetan]除了theta0, interception代表theta0"""
      """生成一列为1的向量，长度与训练数据相同进行合并，目的是为了凑theta0的X，使每一项都存在X"""
      X_b = np.hstack([np.ones((len(X_train),1)), X_train]) 
      self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)
      self.interception_ = self._theta[0]
      self.coef_ = self._theta[1:]
    ```
    - 计算得出的theta这个模型得分r2方程组
    ```py
      def score(self, X_test, y_test):
        y_predict = self.predict(X_test)
        return 1 - mean_squared_error(y_test, y_predict) / np.var(y_test)
    ```
    
  ### 梯度下降法
    - 作用，最小化损失函数的搜索方法
    - 对y=损失函数与x=theta模型的求导获取损失函数的极值点最小值时
    - 对y=损失函数与x=theta模型的求导，导数可代表方向，导数为负表示损失函数在X轴负方向上递增
    - eta表示步长 (学习率，梯度下降法的超参数)，-eta * dJ * dtheta 表示单位步长。eta太小影响计算效率，太大导致损失函数反而上升没有计算结果
    - 并不是所有y=损失函数与x=theta模型都有唯一的极小值，初始点也是梯度下降的超参数
      - 解决方法： 每次随机初始点，多次运行
    - 向量化和数据标准化：数据的单位要以归一法进行处理
    - 梯度下降法的优化。可用随机梯度下降法进行
     
  ### 主成分分析法，梯度分析法获取最佳的w单位向量
    - 主要作用，降维，降噪，可视化
      - 降维：多维数据如二维数据的点映射到一条直线上，该数据集的方处于最大值，使尽可能保留数据。
      - 降噪：降低维度的同时，数据集中到一其他维度，如二维数据映射到一条直线上，虽丢失了信息，但可能丢失了更多无效数据，增加了精度
    - 步骤1: 样本均值归0 demean
    - 步骤2: 方差最大值
    - 计算得到第一主成分分量，将demean后的数据减去第一主成分分量后即可计算其他成分，循环往复
    ```py
    # sklearn中的使用
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn import datasets
    digits = datasets.load_digits()
    X = digits.data
    y = digits.target
    from sklearn.model_selection import train_test_split # 导入数据拆分
    X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=666)
    from sklearn.neighbors import KNeighborsClassifier
    knn_clf1 = KNeighborsClassifier()
    X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=666)
    knn_clf1.fit(X_train1,y_train1)
    knn_clf1.score(X_test1,y_test1) # time= 20ms, 98.5%
    
    #pca降维降噪后
    from sklearn.decomposition import PCA
    pca = PCA(0.95)
    pca.fit(X_train1)
    X_train_reducation1 = pca.transform(X_train1)
    X_test_reducation1 = pca.transform(X_test1)
    knn_clf1.fit(X_train_reducation1, y_train)
    knn_clf1.score(X_test_reducation1,y_test1) # time = 10ms 98%
    ```
    
